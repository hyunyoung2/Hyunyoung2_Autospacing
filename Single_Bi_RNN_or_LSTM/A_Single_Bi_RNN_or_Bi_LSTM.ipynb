{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n",
      "Tensorflow version: 1.8.0\n",
      "Numpy version: 1.14.3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This code is for example code about A single Bi-RNN(Bi-LSTM) Practice by Hyunyoung2\n",
    "\n",
    "the key of this code is summation between outputs of forwards and backwards.\n",
    "\n",
    "The code is made to understand the rnn execution on tensorflow by me.\n",
    "\n",
    "Also this is Basic version for Autospacing about Korean Language.\n",
    "\n",
    "Be careful about this information about the following code,\n",
    "\n",
    "This code only run on 1 batch, So you would have to deal with\n",
    "\n",
    "one sentence by one sentence to automatically space a sentence.\n",
    "\"\"\"\n",
    "### Version Check For tensorflow ###\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# for generating a batch\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "# For data setting for tensorflow graph ##\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Python version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "print(\"Numpy version: {}\".format(np.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== The Original Data =======\n",
      "The total vocabularies: ['학교에갔었다', '서울에도착', '학교에', '서울']\n",
      "The total labels: ['BIIBII', 'BIIBI', 'BII', 'BI']\n",
      "===== The Original Data =======\n",
      "Data: ['학교에갔었다', '서울에도착', '학교에', '서울']\n",
      "Ground Truths: ['BIIBII', 'BIIBI', 'BII', 'BI']\n",
      "===== The Training Data =======\n",
      "Data: ['학교에갔었다', '서울에도착', '학교에', '서울']\n",
      "Ground Truths: ['BIIBII', 'BIIBI', 'BII', 'BI']\n",
      "===== The Evaluating Data =======\n",
      "Data: ['학교에갔었다', '서울에도착', '학교에', '서울']\n",
      "Ground Truths: ['BIIBII', 'BIIBI', 'BII', 'BI']\n",
      "===== The predicting Data =======\n",
      "Data: ['학교에갔었다', '서울에도착', '학교에', '서울']\n",
      "Ground Truths: ['BIIBII', 'BIIBI', 'BII', 'BI']\n"
     ]
    }
   ],
   "source": [
    "### Data setting for Tensorflow learning, evaluating, and predicting ###\n",
    "\n",
    "# For embedding and predicting \n",
    "total_voca = None\n",
    "total_label = None\n",
    "\n",
    "# For training \n",
    "training_data = None\n",
    "training_label = None\n",
    "\n",
    "# For evaluating\n",
    "evaluating_data = None\n",
    "evaluating_label = None\n",
    "\n",
    "# For predicting\n",
    "predicting_data = None\n",
    "predicting_label = None\n",
    "\n",
    "# The original data\n",
    "\n",
    "data = [\"학교에갔었다\", \n",
    "       \"서울에도착\",\n",
    "       \"학교에\",\n",
    "       \"서울\"]\n",
    "\n",
    "ground_truths = [\"BIIBII\",\n",
    "                \"BIIBI\",\n",
    "                \"BII\",\n",
    "                \"BI\"]\n",
    "\n",
    "total_voca = training_data = evaluating_data = predicting_data = data\n",
    "total_label = training_label = evaluating_label = predicting_label = ground_truths\n",
    "\n",
    "## The total voca and label ###\n",
    "print(\"===== The Original Data =======\")\n",
    "print(\"The total vocabularies: {}\".format(total_voca))\n",
    "print(\"The total labels: {}\".format(total_label))\n",
    "\n",
    "## The original data ###\n",
    "print(\"===== The Original Data =======\")\n",
    "print(\"Data: {}\".format(data))\n",
    "print(\"Ground Truths: {}\".format(ground_truths))\n",
    "\n",
    "## The training data ###\n",
    "print(\"===== The Training Data =======\")\n",
    "print(\"Data: {}\".format(training_data))\n",
    "print(\"Ground Truths: {}\".format(training_label))\n",
    "\n",
    "## The evaluating data ###\n",
    "print(\"===== The Evaluating Data =======\")\n",
    "print(\"Data: {}\".format(evaluating_data))\n",
    "print(\"Ground Truths: {}\".format(evaluating_label))\n",
    "\n",
    "## The predicting data ###\n",
    "print(\"===== The predicting Data =======\")\n",
    "print(\"Data: {}\".format(predicting_data))\n",
    "print(\"Ground Truths: {}\".format(predicting_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Data =======\n",
      "<class 'list'>\n",
      "Example of data_joined: 학교에갔었다서울에도착학교에서울, type: <class 'str'>\n",
      "Example of data_counter: Counter({'에': 3, '교': 2, '울': 2, '서': 2, '학': 2, '도': 1, '착': 1, '다': 1, '갔': 1, '었': 1}), type: <class 'collections.Counter'>, len: 10\n",
      "The list of data: ['도', '교', '울', '서', '착', '에', '다', '갔', '었', '학'], len: 10, type: <class 'list'>\n",
      "data2idx: {'착': 4, '울': 2, '서': 3, '었': 8, '에': 5, '다': 6, '교': 1, '갔': 7, '학': 9, '도': 0}, len: 10, type: <class 'dict'>\n",
      "idx2data: {0: '도', 1: '교', 2: '울', 3: '서', 4: '착', 5: '에', 6: '다', 7: '갔', 8: '었', 9: '학'}, len: 10, type: <class 'dict'>\n",
      "\n",
      "\n",
      "===== Label =======\n",
      "<class 'list'>\n",
      "Example of data_joined: BIIBIIBIIBIBIIBI, type: <class 'str'>\n",
      "Example of data_counter: Counter({'I': 10, 'B': 6}), type: <class 'collections.Counter'>, len: 2\n",
      "The list of data: ['I', 'B'], len: 2, type: <class 'list'>\n",
      "data2idx: {'I': 0, 'B': 1}, len: 2, type: <class 'dict'>\n",
      "idx2data: {0: 'I', 1: 'B'}, len: 2, type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "### Generating Dictionary ####\n",
    "def generate_feature_dict(total):\n",
    "    print(type(total))\n",
    "    # for Counter, data is join\n",
    "    data_joined = \"\".join(total)\n",
    "    print(\"Example of data_joined: {}, type: {}\".format(data_joined,\n",
    "                                                        type(data_joined)))\n",
    "    #del data\n",
    "    \n",
    "    data_counter = Counter(data_joined)\n",
    "    # removal of the duplication of syllable\n",
    "    data_counter = Counter(data_joined)\n",
    "    print(\"Example of data_counter: {}, type: {}, len: {}\".format(data_counter,\n",
    "                                                                   type(data_counter),\n",
    "                                                                   len(data_counter)))\n",
    "    del data_joined\n",
    "    # make id per data, for example, data could be word and syllable\n",
    "    # the index of a list is \n",
    "    data_list = [val for key, val in enumerate(data_counter)]\n",
    "    # if you want to data list for idx per data, you don't need to use idx2data dict.\n",
    "    # just use like data_list[data2idx(\"data\")]\n",
    "    # But here I used two the dictionaries for indexing for data, idx number.\n",
    "    data2idx = {val:idx for idx, val in enumerate(data_list)}\n",
    "    idx2data = {idx:val for idx, val in enumerate(data_list)}\n",
    "    print(\"The list of data: {}, len: {}, type: {}\".format(data_list,\n",
    "                                                           len(data_list),\n",
    "                                                           type(data_list)))\n",
    "    print(\"data2idx: {}, len: {}, type: {}\".format(data2idx, \n",
    "                                                   len(data2idx),\n",
    "                                                   type(data2idx)))\n",
    "    print(\"idx2data: {}, len: {}, type: {}\".format(idx2data,\n",
    "                                                   len(idx2data),\n",
    "                                                   type(idx2data)))\n",
    "    del data_list\n",
    "    \n",
    "    return data2idx, idx2data\n",
    "\n",
    "# for data \n",
    "print(\"===== Data =======\")\n",
    "data2idx, idx2data = generate_feature_dict(total_voca)\n",
    "\n",
    "# for label\n",
    "print(\"\\n\\n===== Label =======\")\n",
    "label2idx, idx2label = generate_feature_dict(total_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== The Training Data =======\n",
      "Data: [[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4], [9, 1, 5], [3, 2]]\n",
      "change_label: [[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0], [1, 0]]\n",
      "===== The Evaluating Data =======\n",
      "Data: [[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4], [9, 1, 5], [3, 2]]\n",
      "change_label: [[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0], [1, 0]]\n",
      "===== The predicting Data =======\n",
      "Data: [[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4], [9, 1, 5], [3, 2]]\n",
      "change_label: [[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0], [1, 0]]\n"
     ]
    }
   ],
   "source": [
    "### Make training data for tensorflow ####\n",
    "def preprecossing_raw_data(data, dict_for_data):\n",
    "    #print(\"data: {}\".format(data))\n",
    "    #print(\"dict_for_data: {}\".format(dict_for_data))\n",
    "    \n",
    "    data_returned = list()\n",
    "    \n",
    "    # data is 2-dimensional for this function\n",
    "    for idx, val in enumerate(data):\n",
    "        # indexing_data is 1-dimensional\n",
    "        indexing_data = list()\n",
    "        for idx2, val2 in enumerate(val):\n",
    "            indexing_data.append(dict_for_data[val2])\n",
    "        \n",
    "        data_returned.append(indexing_data)\n",
    "    \n",
    "    return data_returned\n",
    "\n",
    "def change_label(label):\n",
    "    \n",
    "    label_changed = list()\n",
    "    \n",
    "    for idx, val in enumerate(label):\n",
    "        temp_label = list()\n",
    "        for idx2, val2 in enumerate(val):\n",
    "            if val2 == 1:  # B tag\n",
    "                temp_label.append([1, 0])\n",
    "            else: # I tag\n",
    "                temp_label.append([0, 1])\n",
    "        label_changed.append(temp_label)\n",
    "    \n",
    "    return label_changed\n",
    "        \n",
    "\n",
    "# for training \n",
    "x_train = preprecossing_raw_data(training_data, data2idx)\n",
    "y_train = preprecossing_raw_data(training_label, label2idx)\n",
    "#y_train = change_label(y_train_before)\n",
    "\n",
    "# for evaluating\n",
    "x_evaluating = preprecossing_raw_data(evaluating_data, data2idx)\n",
    "y_evaluating = preprecossing_raw_data(evaluating_label, label2idx)\n",
    "#y_evaluating = change_label(y_evaluating_before)\n",
    "\n",
    "# for predicting\n",
    "x_predicting = preprecossing_raw_data(predicting_data, data2idx)\n",
    "y_predicting = preprecossing_raw_data(predicting_label, label2idx)\n",
    "#y_predicting = change_label(y_predicting_before)\n",
    "\n",
    "## The training data ###\n",
    "print(\"===== The Training Data =======\")\n",
    "print(\"Data: {}\".format(x_train))\n",
    "#print(\"Ground Truths: {}\".format(y_train_before))\n",
    "print(\"change_label: {}\".format(y_train))\n",
    "\n",
    "## The evaluating data ###\n",
    "print(\"===== The Evaluating Data =======\")\n",
    "print(\"Data: {}\".format(x_evaluating))\n",
    "#print(\"Ground Truths: {}\".format(y_evaluating_before))\n",
    "print(\"change_label: {}\".format(y_evaluating))\n",
    "\n",
    "## The predicting data ###\n",
    "print(\"===== The predicting Data =======\")\n",
    "print(\"Data: {}\".format(x_predicting))\n",
    "#print(\"Ground Truths: {}\".format(y_predicting_before))\n",
    "print(\"change_label: {}\".format(y_predicting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== The Training Data =======\n",
      "========== suffling ============\n",
      "x: [[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4], [9, 1, 5], [3, 2]]\n",
      "y: [[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0], [1, 0]]\n",
      "========== Batching :1 ============\n",
      "x: [[[9, 1, 5, 7, 8, 6]], [[3, 2, 5, 0, 4]], [[9, 1, 5]], [[3, 2]]]\n",
      "y: [[[1, 0, 0, 1, 0, 0]], [[1, 0, 0, 1, 0]], [[1, 0, 0]], [[1, 0]]]\n",
      "========== Batching :2 ============\n",
      "x: [[[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4]], [[3, 2, 5, 0, 4], [9, 1, 5]]]\n",
      "y: [[[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0]], [[1, 0, 0, 1, 0], [1, 0, 0]]]\n",
      "===== The Evaluating Data =======\n",
      "========== suffling ============\n",
      "x: [[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4], [9, 1, 5], [3, 2]]\n",
      "y: [[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0], [1, 0]]\n",
      "========== Batching :1 ============\n",
      "x: [[[9, 1, 5, 7, 8, 6]], [[3, 2, 5, 0, 4]], [[9, 1, 5]], [[3, 2]]]\n",
      "y: [[[1, 0, 0, 1, 0, 0]], [[1, 0, 0, 1, 0]], [[1, 0, 0]], [[1, 0]]]\n",
      "========== Batching :2 ============\n",
      "x: [[[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4]], [[3, 2, 5, 0, 4], [9, 1, 5]]]\n",
      "y: [[[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0]], [[1, 0, 0, 1, 0], [1, 0, 0]]]\n",
      "===== The predicting Data =======\n",
      "========== suffling ============\n",
      "x: [[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4], [9, 1, 5], [3, 2]]\n",
      "y: [[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0], [1, 0]]\n",
      "========== Batching :1 ============\n",
      "x: [[[9, 1, 5, 7, 8, 6]], [[3, 2, 5, 0, 4]], [[9, 1, 5]], [[3, 2]]]\n",
      "y: [[[1, 0, 0, 1, 0, 0]], [[1, 0, 0, 1, 0]], [[1, 0, 0]], [[1, 0]]]\n",
      "========== Batching :2 ============\n",
      "x: [[[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4]], [[3, 2, 5, 0, 4], [9, 1, 5]]]\n",
      "y: [[[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0]], [[1, 0, 0, 1, 0], [1, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "### Generate a batch ####\n",
    "\n",
    "# shuffle function randomly\n",
    "# if shuffle is true, shuffle \n",
    "# if not, don't shuffle\n",
    "def shuffle_data(x_data, y_label, shuffle=False, debugging=False):\n",
    "    x_data_shuffled = list()\n",
    "    y_label_shuffled = list()\n",
    "    \n",
    "    data_zip = list(zip(x_data, y_label))\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.shuffle(data_zip)\n",
    "        \n",
    "    for idx, val in enumerate(data_zip):\n",
    "        x_data_shuffled.append(val[0])\n",
    "        y_label_shuffled.append(val[1])\n",
    "    \n",
    "    if debugging: \n",
    "        print(\"========== suffling ============\")\n",
    "        print(\"x: {}\".format(x_data_shuffled))\n",
    "        print(\"y: {}\".format(y_label_shuffled))\n",
    "    \n",
    "    return x_data_shuffled, y_label_shuffled\n",
    "\n",
    "def generate_a_batch(x_data, y_label, batch_size=1, debugging=False):\n",
    "    assert len(x_data) % batch_size == 0 \n",
    "    \n",
    "    batch_x_data = list()\n",
    "    batch_y_label = list()\n",
    "    \n",
    "    for idx in range(len(x_data)//batch_size):\n",
    "        batch_x_data.append(x_data[idx:idx+batch_size])\n",
    "        batch_y_label.append(y_label[idx:idx+batch_size])\n",
    "    \n",
    "    if debugging:\n",
    "        print(\"========== Batching :{} ============\".format(batch_size))\n",
    "        print(\"x: {}\".format(batch_x_data))\n",
    "        print(\"y: {}\".format(batch_y_label))\n",
    "    \n",
    "    return batch_x_data, batch_y_label\n",
    "\n",
    "## The training data ###\n",
    "print(\"===== The Training Data =======\")\n",
    "shuffle_x_training, shuffl_y_training = shuffle_data(x_train, y_train, False, True)\n",
    "batch_x_training1, batch_y_training1 = generate_a_batch(shuffle_x_training, shuffl_y_training, 1, True)\n",
    "batch_x_training2, batch_y_training2 = generate_a_batch(shuffle_x_training, shuffl_y_training, 2, True)\n",
    "## The evaluating data ###\n",
    "print(\"===== The Evaluating Data =======\")\n",
    "shuffle_x_evaluating, shuffl_y_evaluating = shuffle_data(x_evaluating, y_evaluating, False, True)\n",
    "batch_x_evaluating1, batch_y_evaluating1 = generate_a_batch(shuffle_x_evaluating, shuffl_y_evaluating, 1, True)\n",
    "batch_x_evaluating2, batch_y_evaluating2 = generate_a_batch(shuffle_x_evaluating, shuffl_y_evaluating, 2, True)\n",
    "\n",
    "## The predicting data ###\n",
    "print(\"===== The predicting Data =======\")\n",
    "shuffle_x_predicting, shuffl_y_predicting = shuffle_data(x_predicting, y_predicting, False, True)\n",
    "batch_x_evaluating1, batch_y_evaluating1 = generate_a_batch(shuffle_x_evaluating, shuffl_y_evaluating, 1, True)\n",
    "batch_x_predicting2, batch_y_predicting2 = generate_a_batch(shuffle_x_predicting, shuffl_y_predicting, 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vacabulary size: 10\n",
      "The shape of new_batch_size:()\n",
      "The shape of label_matrix:(2, 2)\n",
      "The shape of label_ids:(?, ?)\n",
      "The shape of label_embeddings:(?, ?, 2)\n",
      "The shape of embedding_matrix:(10, 2)\n",
      "The shape of word_ids:(?, ?)\n",
      "The shape of word_embeddings:(?, ?, 2)\n",
      "The shape of x_inputs:(?, ?, 2)\n",
      "The shape of y_labels:(?, ?, 2)\n",
      "Cell type: BasicLSTMCell\n",
      "Cell type: BasicLSTMCell\n",
      "The my_fw_rnn_cell:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f8bff4140b8>\n",
      "The my_bw_rnn_cell:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f8bff414358>\n",
      "The output_fw:Tensor(\"bidirectional_rnn_1/fw/fw/transpose_1:0\", shape=(?, ?, 2), dtype=float32)\n",
      "The output_bw:Tensor(\"ReverseV2_1:0\", shape=(?, ?, 2), dtype=float32)\n",
      "The output_states:(LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn_1/fw/fw/while/Exit_3:0' shape=(?, 2) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn_1/fw/fw/while/Exit_4:0' shape=(?, 2) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn_1/bw/bw/while/Exit_3:0' shape=(?, 2) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn_1/bw/bw/while/Exit_4:0' shape=(?, 2) dtype=float32>))\n",
      "The shape of predictions:(?, ?, 2)\n",
      "===== variable type =====\n",
      "tf.GraphKeys.GLOBAL_VARIABLES: [[], []]\n",
      "tf.GraphKeys.TRAINABLE_VARIABLES: [[], []]\n",
      "tf.GraphKeys.LOCAL_VARIABLES: [[], []]\n",
      "\n",
      "\n",
      "===== all variables =====\n",
      "Embedding_matrix:0\n",
      "bidirectional_rnn/fw/BasicLSTMCell/kernel:0\n",
      "bidirectional_rnn/fw/BasicLSTMCell/bias:0\n",
      "bidirectional_rnn/bw/BasicLSTMCell/kernel:0\n",
      "bidirectional_rnn/bw/BasicLSTMCell/bias:0\n"
     ]
    }
   ],
   "source": [
    "### Build Grapah for Tensorflow ###\n",
    "\n",
    "############################\n",
    "# Graph's Hyper parameters #\n",
    "############################\n",
    "\n",
    "# For word embedding\n",
    "vocabulary_size = len(data2idx) # here, the number of syllable\n",
    "print(\"Vacabulary size: {}\".format(vocabulary_size))\n",
    "embedding_size = 2\n",
    "\n",
    "# a RNN and LSTM\n",
    "batch_sizes = 1\n",
    "time_steps = None\n",
    "num_feature = embedding_size\n",
    "# for add or concatenation of fw, bw \n",
    "# The current version only uses add option  \n",
    "add_or_concat = [\"add\", \"concatenation\"] \n",
    "Selected_ouput_option = add_or_concat[0]\n",
    "\n",
    "# for Output Layer\n",
    "num_classes = len(label2idx) # here, the number of tags, B and I, i.e.2\n",
    "\n",
    "# for cell\n",
    "num_hidden_units = num_classes #embedding_size\n",
    "\n",
    "# for learning parameter\n",
    "learning_rate = 0.1\n",
    "epoch = 1\n",
    "steps = 4 # total data size // batch size\n",
    "log_location = \"./log\"\n",
    "\n",
    "############################\n",
    "#    Graph's input part    #\n",
    "############################\n",
    "\n",
    "new_batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "\n",
    "word_ids = tf.placeholder(tf.int32, (None, None), name=\"word_ids\") # x data in (batch, time)\n",
    "label_ids = tf.placeholder(tf.int32, (None, None), name=\"label_ids\")\n",
    "\n",
    "embedding_matrix = tf.get_variable(\"Embedding_matrix\", shape=[vocabulary_size, embedding_size], dtype=tf.float32)\n",
    "word_embeddings = tf.nn.embedding_lookup(embedding_matrix, word_ids, name=\"word_embedding_lookup\")\n",
    "\n",
    "# for sparse label data\n",
    "label_matrix = tf.constant([[1, 0],[0, 1]], dtype=tf.float32, name=\"label_matrix\")\n",
    "label_embeddings = tf.nn.embedding_lookup(label_matrix, label_ids, name=\"label_embedding_lookup\")\n",
    "\n",
    "\n",
    "x_inputs = word_embeddings # The original version = tf.placeholder(tf.float32, (None, None, num_features), name=\"Input\") # (batch, time, in)\n",
    "# tf.float32 is for cross entropy cost function.\n",
    "y_labels = label_embeddings # tf.placeholder(tf.float32, (batch_sizes , time_steps, num_classes), name=\"output_label\") # (batch, time, output)\n",
    "\n",
    "#print(\"The shape of y_labels:{}\".format(y_labels.shape))\n",
    "print(\"The shape of new_batch_size:{}\".format(new_batch_size.shape))\n",
    "\n",
    "print(\"The shape of label_matrix:{}\".format(label_matrix.shape))\n",
    "print(\"The shape of label_ids:{}\".format(label_ids.shape))\n",
    "print(\"The shape of label_embeddings:{}\".format(label_embeddings.shape))\n",
    "\n",
    "print(\"The shape of embedding_matrix:{}\".format(embedding_matrix.shape))\n",
    "print(\"The shape of word_ids:{}\".format(word_ids.shape))\n",
    "print(\"The shape of word_embeddings:{}\".format(word_embeddings.shape))\n",
    "\n",
    "print(\"The shape of x_inputs:{}\".format(x_inputs.shape))\n",
    "print(\"The shape of y_labels:{}\".format(y_labels.shape))\n",
    "\n",
    "############################\n",
    "#        Graph's RNN       #\n",
    "############################\n",
    "\n",
    "# defined in https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/rnn_cell_impl.py\n",
    "rnn_type = [\"BasicRNNCell\", \"BasicLSTMCell\"]\n",
    "\n",
    "# If you want to use another RNN, just change the following rnn_type\n",
    "selected_rnn_type = rnn_type[1]\n",
    "\n",
    "def make_cell(selected, cell_units):\n",
    "    \n",
    "    cell = None # \n",
    "    \n",
    "    if selected == rnn_type[0]: \n",
    "        # Aliases : tf.contrib.rnn.BasicRNNCell\n",
    "        # Most basic RNN: output = new_state = act(W * input(X_vector) + U * state(hidden) + B) in call function\n",
    "        cell = tf.nn.rnn_cell.BasicRNNCell(cell_units, name=rnn_type[0])\n",
    "    \n",
    "    elif selected == rnn_type[1]:\n",
    "        # Aliases : tf.contrib.BasicLSTMCell \n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(cell_units, name=rnn_type[1])\n",
    "        \n",
    "    print(\"Cell type: {}\".format(selected))\n",
    "    return cell\n",
    "\n",
    "# If you want to make bidirectional RNN, you need two cell for forwards and backwards\n",
    "# make multiRNN\n",
    "my_fw_rnn_cell = make_cell(selected_rnn_type, num_hidden_units) \n",
    "my_bw_rnn_cell = make_cell(selected_rnn_type, num_hidden_units) \n",
    "\n",
    "# You don't need to initialize the initail_state, if you want zero_state\n",
    "# that is by default zero_state\n",
    "if selected_rnn_type == rnn_type[1]:\n",
    "    my_initial_fw = my_fw_rnn_cell.zero_state(new_batch_size, dtype=tf.float32)\n",
    "    my_initial_bw = my_bw_rnn_cell.zero_state(new_batch_size, dtype=tf.float32)\n",
    "\n",
    "else:\n",
    "    my_initial_fw = tf.zeros([new_batch_size, my_fw_rnn_cell.state_size], dtype=tf.float32)\n",
    "    my_initial_bw = tf.zeros([new_batch_size, my_bw_rnn_cell.state_size], dtype=tf.float32)\n",
    "    \n",
    "#(output_fw, output_bw), output_states\n",
    "outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=my_fw_rnn_cell,\n",
    "                                                                        cell_bw=my_bw_rnn_cell,\n",
    "                                                                        inputs=x_inputs,\n",
    "                                                                        sequence_length=None,\n",
    "                                                                        initial_state_fw=my_initial_fw,\n",
    "                                                                        initial_state_bw=my_initial_bw,\n",
    "                                                                        dtype=tf.float32)\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn\n",
    "# return values = outputs and output_states\n",
    "(output_fw, output_bw), output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=my_fw_rnn_cell,\n",
    "                                                                        cell_bw=my_bw_rnn_cell,\n",
    "                                                                        inputs=x_inputs,\n",
    "                                                                        sequence_length=None,\n",
    "                                                                        initial_state_fw=my_initial_fw,\n",
    "                                                                        initial_state_bw=my_initial_bw,\n",
    "                                                                        dtype=tf.float32)\n",
    "\n",
    "\n",
    "print(\"The my_fw_rnn_cell:{}\".format(my_fw_rnn_cell))\n",
    "print(\"The my_bw_rnn_cell:{}\".format(my_bw_rnn_cell))\n",
    "print(\"The output_fw:{}\".format(output_fw))\n",
    "print(\"The output_bw:{}\".format(output_bw))\n",
    "print(\"The output_states:{}\".format(output_states))\n",
    "\n",
    "############################\n",
    "#  Graph's Classfication   #\n",
    "############################\n",
    "\n",
    "# in here, we don't need to classify them, just use probability distribution\n",
    "\n",
    "#softmax_outputs_of_rnn = tf.nn.softmax(outputs)\n",
    "\n",
    "\n",
    "if Selected_ouput_option == \"add\":\n",
    "    predictions = tf.add(output_fw, output_bw)\n",
    "\n",
    "print(\"The shape of predictions:{}\".format(predictions.shape))\n",
    "\n",
    "############################\n",
    "#      Graph's Loss        #\n",
    "############################\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2\n",
    "loss_op = tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=y_labels)\n",
    "loss_mean_op = tf.reduce_mean(loss_op)\n",
    "tf.summary.scalar(\"Loss\", loss_mean_op)\n",
    "\n",
    "############################\n",
    "#    Graph's optimizer     #\n",
    "############################\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "traing_op = optimizer.minimize(loss_mean_op)\n",
    "\n",
    "\n",
    "############################\n",
    "#    Graph's Accuracy      #\n",
    "############################\n",
    "\n",
    "# predictions_prob_distributed means the probability to each label. \n",
    "predictions_prob_distributed = tf.nn.softmax(predictions)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(predictions_prob_distributed, 2), tf.argmax(y_labels, 2))\n",
    "true_or_not = tf.cast(correct_prediction, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(true_or_not)\n",
    "accuracy_per_line = tf.reduce_sum(true_or_not)\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "############################\n",
    "#    Graph's Prediction    #\n",
    "############################\n",
    "\n",
    "# Keep in mind that thit prediction is for one sentence by one sentence\n",
    "\n",
    "# the original tensor is a tensor of shape (batch, time step, output class)\n",
    "# the tensor reshapes the original one to (batch, output class)\n",
    "pro_reshaped = tf.reshape(predictions_prob_distributed, [-1, 2], name=\"To_predict\")\n",
    "\n",
    "tag_predicted = tf.argmax(pro_reshaped, 1)\n",
    "\n",
    "######################################\n",
    "#    Graph's initializing variable   #\n",
    "######################################\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "######################################\n",
    "# Graph's summary merging operation  #\n",
    "######################################\n",
    "\n",
    "merged_op = tf.summary.merge_all()\n",
    "\n",
    "###################################\n",
    "#    Graph's Checking variable    #\n",
    "###################################\n",
    "trainable_variable1 = tf.get_collection(\"tf.GraphKeys.GLOBAL_VARIABLES\")\n",
    "trainable_variable2 = tf.get_collection(\"tf.GraphKeys.TRAINABLE_VARIABLES\")\n",
    "trainable_variable3 = tf.get_collection(\"tf.GraphKeys.LOCAL_VARIABLES\")\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "print(\"===== variable type =====\")\n",
    "print(\"tf.GraphKeys.GLOBAL_VARIABLES: {}\".format(sess.run([trainable_variable1,trainable_variable1])))\n",
    "print(\"tf.GraphKeys.TRAINABLE_VARIABLES: {}\".format(sess.run([trainable_variable1,trainable_variable2])))\n",
    "print(\"tf.GraphKeys.LOCAL_VARIABLES: {}\".format(sess.run([trainable_variable1,trainable_variable3])))\n",
    "\n",
    "print(\"\\n\\n===== all variables =====\")\n",
    "for v in tf.global_variables():\n",
    "    print(v.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Let's train!!==============\n",
      "========== suffling ============\n",
      "x: [[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4], [9, 1, 5], [3, 2]]\n",
      "y: [[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0], [1, 0]]\n",
      "========== Batching :1 ============\n",
      "x: [[[9, 1, 5, 7, 8, 6]], [[3, 2, 5, 0, 4]], [[9, 1, 5]], [[3, 2]]]\n",
      "y: [[[1, 0, 0, 1, 0, 0]], [[1, 0, 0, 1, 0]], [[1, 0, 0]], [[1, 0]]]\n",
      "The epoch: 0\n",
      "The number of batchs: 4\n",
      "====================== my batch data ============================\n",
      "idx: 0, x: [[9, 1, 5, 7, 8, 6]], y: [[1, 0, 0, 1, 0, 0]]\n",
      "loss_mean_op:0.7045660018920898\n",
      "====================== my batch data ============================\n",
      "idx: 1, x: [[3, 2, 5, 0, 4]], y: [[1, 0, 0, 1, 0]]\n",
      "loss_mean_op:0.6986664533615112\n",
      "====================== my batch data ============================\n",
      "idx: 2, x: [[9, 1, 5]], y: [[1, 0, 0]]\n",
      "loss_mean_op:0.6686927676200867\n",
      "====================== my batch data ============================\n",
      "idx: 3, x: [[3, 2]], y: [[1, 0]]\n",
      "loss_mean_op:0.678754448890686\n",
      "\n",
      "\n",
      "================ Let's evaluate!!==============\n",
      "========== suffling ============\n",
      "x: [[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4], [9, 1, 5], [3, 2]]\n",
      "y: [[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0], [1, 0]]\n",
      "========== Batching :1 ============\n",
      "x: [[[9, 1, 5, 7, 8, 6]], [[3, 2, 5, 0, 4]], [[9, 1, 5]], [[3, 2]]]\n",
      "y: [[[1, 0, 0, 1, 0, 0]], [[1, 0, 0, 1, 0]], [[1, 0, 0]], [[1, 0]]]\n",
      "The epoch: 0\n",
      "The number of batchs: 4\n",
      "====================== my batch data ============================\n",
      "idx: 0, x: [[9, 1, 5, 7, 8, 6]], y: [[1, 0, 0, 1, 0, 0]]\n",
      "Accuracy for each sentence in data: 0.5\n",
      "====================== my batch data ============================\n",
      "idx: 1, x: [[3, 2, 5, 0, 4]], y: [[1, 0, 0, 1, 0]]\n",
      "Accuracy for each sentence in data: 0.4000000059604645\n",
      "====================== my batch data ============================\n",
      "idx: 2, x: [[9, 1, 5]], y: [[1, 0, 0]]\n",
      "Accuracy for each sentence in data: 0.6666666865348816\n",
      "====================== my batch data ============================\n",
      "idx: 3, x: [[3, 2]], y: [[1, 0]]\n",
      "Accuracy for each sentence in data: 0.5\n",
      "The current epoch(0) accuracy: 0.5\n",
      "\n",
      "\n",
      "================ Let's predict!!==============\n",
      "========== suffling ============\n",
      "x: [[9, 1, 5, 7, 8, 6], [3, 2, 5, 0, 4], [9, 1, 5], [3, 2]]\n",
      "y: [[1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0], [1, 0, 0], [1, 0]]\n",
      "========== Batching :1 ============\n",
      "x: [[[9, 1, 5, 7, 8, 6]], [[3, 2, 5, 0, 4]], [[9, 1, 5]], [[3, 2]]]\n",
      "y: [[[1, 0, 0, 1, 0, 0]], [[1, 0, 0, 1, 0]], [[1, 0, 0]], [[1, 0]]]\n",
      "The epoch: 0\n",
      "The number of batchs: 4\n",
      "====================== my batch data ============================\n",
      "idx: 0, x: [[9, 1, 5, 7, 8, 6]], y: [[1, 0, 0, 1, 0, 0]]\n",
      "prediction:[1 1 0 0 1 0]\n",
      "====================== my batch data ============================\n",
      "idx: 1, x: [[3, 2, 5, 0, 4]], y: [[1, 0, 0, 1, 0]]\n",
      "prediction:[1 1 0 0 1]\n",
      "====================== my batch data ============================\n",
      "idx: 2, x: [[9, 1, 5]], y: [[1, 0, 0]]\n",
      "prediction:[1 1 0]\n",
      "====================== my batch data ============================\n",
      "idx: 3, x: [[3, 2]], y: [[1, 0]]\n",
      "prediction:[1 1]\n"
     ]
    }
   ],
   "source": [
    "#### learning Tenssorflow  ####\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(log_location+\"/train\", sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(log_location+\"/test\")\n",
    "    prediction_writer = tf.summary.FileWriter(log_location+\"/prediction\")\n",
    "    \n",
    "    # Let's training\n",
    "    print(\"================Let's train!!==============\")\n",
    "    for current_epoch in range(epoch):\n",
    "        shuffle_x_training, shuffl_y_training = shuffle_data(x_train, y_train, \n",
    "                                                             False, True)\n",
    "        batch_x_training, batch_y_training = generate_a_batch(shuffle_x_training, \n",
    "                                                               shuffl_y_training, \n",
    "                                                               batch_sizes, True)\n",
    "        print(\"The epoch: {}\".format(current_epoch))\n",
    "        print(\"The number of batchs: {}\".format(len(batch_x_training)))\n",
    "        for idx in range(len(batch_x_training)):\n",
    "            print(\"====================== my batch data ============================\")\n",
    "            print(\"idx: {}, x: {}, y: {}\".format(idx, batch_x_training[idx], batch_y_training[idx]))\n",
    "            summary, loss_mean_op_, _ = sess.run([merged_op, loss_mean_op, traing_op], \n",
    "                                                 feed_dict={new_batch_size: batch_sizes,\n",
    "                                                            word_ids: batch_x_training[idx],\n",
    "                                                            label_ids: batch_y_training[idx]})\n",
    "            \n",
    "            print(\"loss_mean_op:{}\".format(loss_mean_op_))\n",
    "            train_writer.add_summary(summary, idx)\n",
    "        \n",
    "        \n",
    "    # Let's evaluting\n",
    "    print(\"\\n\\n================ Let's evaluate!!==============\")\n",
    "    for current_epoch in range(epoch):\n",
    "        shuffle_x_evaluating, shuffl_y_evaluating = shuffle_data(x_evaluating, y_evaluating, \n",
    "                                                             False, True)\n",
    "        batch_x_evaluating, batch_y_evaluating = generate_a_batch(shuffle_x_evaluating, \n",
    "                                                               shuffl_y_evaluating, \n",
    "                                                               batch_sizes, True)\n",
    "        print(\"The epoch: {}\".format(current_epoch))\n",
    "        print(\"The number of batchs: {}\".format(len(batch_x_evaluating)))\n",
    "        total_accuracy = 0.0\n",
    "        len_label = 0\n",
    "        for idx in range(len(batch_x_evaluating)):\n",
    "            print(\"====================== my batch data ============================\")\n",
    "            print(\"idx: {}, x: {}, y: {}\".format(idx, batch_x_evaluating[idx], batch_y_evaluating[idx]))\n",
    "            len_label += len(batch_y_evaluating[idx][0])\n",
    "            \n",
    "            summary, accuracy_, accuracy_per_line_ = sess.run([merged_op,accuracy, accuracy_per_line], \n",
    "                                                     feed_dict={new_batch_size: batch_sizes, \n",
    "                                                                word_ids: batch_x_evaluating[idx], \n",
    "                                                                label_ids: batch_y_evaluating[idx]})\n",
    "        \n",
    "            print(\"Accuracy for each sentence in data: {}\".format(accuracy_))\n",
    "            #print(\"len_label:{}\".format(len_label))\n",
    "            total_accuracy += accuracy_per_line_\n",
    "            test_writer.add_summary(summary, idx)\n",
    "            \n",
    "        print(\"The current epoch({}) accuracy: {}\".format(current_epoch, (total_accuracy/len_label)))\n",
    "        \n",
    "    # Let's predict\n",
    "    print(\"\\n\\n================ Let's predict!!==============\")\n",
    "    for current_epoch in range(epoch):\n",
    "        shuffle_x_predicting, shuffl_y_predicting = shuffle_data(x_predicting, y_predicting, \n",
    "                                                             False, True)\n",
    "        batch_x_predicting, batch_y_predicting = generate_a_batch(shuffle_x_predicting, \n",
    "                                                               shuffl_y_predicting, \n",
    "                                                               batch_sizes, True)\n",
    "        print(\"The epoch: {}\".format(current_epoch))\n",
    "        print(\"The number of batchs: {}\".format(len(batch_x_predicting)))\n",
    "        len_label = 0\n",
    "        for idx in range(len(batch_x_predicting)):\n",
    "            print(\"====================== my batch data ============================\")\n",
    "            print(\"idx: {}, x: {}, y: {}\".format(idx, batch_x_predicting[idx], batch_y_predicting[idx]))\n",
    "            \n",
    "            \n",
    "            summary, tag_predicted_ = sess.run([merged_op, tag_predicted],\n",
    "                                               feed_dict={new_batch_size: batch_sizes, \n",
    "                                                          word_ids: batch_x_predicting[idx], \n",
    "                                                          label_ids: batch_y_evaluating[idx]})\n",
    "            \n",
    "            \n",
    "            print(\"prediction:{}\".format(tag_predicted_))\n",
    "            prediction_writer.add_summary(summary, idx)\n",
    "    \n",
    "    \n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    prediction_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
